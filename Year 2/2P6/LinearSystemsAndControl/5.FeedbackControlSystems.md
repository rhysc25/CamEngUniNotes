If we have a controller K and a plant G, we could try making an arbitrary transfer function by setting $K=F/G$. However this would never work in practice as the knowledge of the transfer function of G is only approximate, and also the effect of disturbances would never allow it to be accurate.
Therefore we need feedback

![[Pasted image 20251107112045.png]]

We know have that $\bar{y}=\frac{GK}{1+HGK}\bar{r}+\frac{1}{1+HGK}\bar{d}_0+\frac{G}{1+HGK}\bar{d}_i$
The term $1+HGK$ appears as the denominator of all the closed-loop transfer functions. Therefore it determines the poles and hence the stability of the system

We call $L(s)=HGK$, where L is the return ratio of the loop or the loop transfer function.
The closed loop characteristic equation is $1+L(s)=0$

If a feedback system has input $r(t)$ and disturbance $d_0(t)$ then we can say that:
$\bar{y}(s)=T(s)\bar{r}(s)+S(s)\bar{d}_0(s)$
Where T(s) is the complementary sensitivity and S(s) is the sensitivity
$S(s)+T(s)=1$ always

**The Final Value Theorem**

Let $y(t)$ be the step response of the system.
As dividing my s in the Laplace domain is equivalent to integrating: $\bar{y}=\frac{G(s)}{s}$

From this we can see that the final value of the step response = transfer function evaluated at $s=0$. This final value is the steady state gain or DC gain
$\lim_{s\rightarrow 0}=G(0)$

The steady state response of the system to a step input that is equal to U is a constant G(0)U
The steady state response to a sinusoidal input is $|G(j\omega)|\cos(\omega t + \arg G(j\omega))$
We can see these are equivalent is $\omega =0$

We can see that the error signal a feedback system is given by $\bar{e}=\frac{1}{1+L(s)}\bar{r}$
If $r(t)=H(t)$ then, using the final value theorem, the steady state error is $\frac{1}{1+L(0)}$

**Control Policies**
**Proportional Control**
$K(s)=k_p$. This increased $L(0)$ so reduces steady state error. However it also means more aggressive control so damping is reduced, and there is a possibility for loss of closed-loop stability
$\omega_n=\sqrt{1+k_p} \qquad \zeta=\frac{1}{\sqrt{1+k_p}}$
As $k_p$ increases, more oscillatory output
Steady state error: $\frac{1}{1+k_p}$

To increase damping - use derivate action or velocity feedback
To remove steady state errors - use integral action

**Proportional + Derivative (PD) Control**
$K(s)=k_p+k_ds$
Increased damping, but greater sensitivity to noise
$\omega_n=\sqrt{1+k_p} \qquad \zeta=\frac{2+k_d}{2\sqrt{1+k_p}}$

**Proportional + Integral (PI) Control**
Steady state error is $\frac{1}{1+G(0)K(0)}$
Therefore to remove steady steady state error, we need $K(0)=\infty$
Therefore $K(s)=k_p+\frac{k_i}{s}$
The integral of the error signal will only ever remain stable if the error signal converges to zero. The only equilibrium possible is one where $\lim_{t\rightarrow \infty} e(t) =0$
Provided the closed loop system is asymptotically stable

**Proportional + Integral + Derivative (PID) Control**
Combines the advantages of both derivative and integral action
$1+G(s)(k_p+\frac{k_i}{s}+k_ds)$
There are many empirical rules for tuning PID controllers (Ziegler-Nichols for example)

![[Pasted image 20251121113324.png]]
The closed loop system is only stable if the Nyquist diagram of the return ratio doesn't enclose the point "-1".

Drawing Nyquist Diagrams:
Find an expression for $G(j\omega)$, and find $|G(j\omega)|$ and $\angle G(j\omega)$
Find how these evaluate as as $\omega \rightarrow 0$ and as $\omega \rightarrow \infty$
If we get the result that $G(j0)\rightarrow \infty$, because of an integrator term, we must use a Taylor series expansion about $\omega = 0$ to more accurately determine the shape

Nyquist's Stability Theorem allows us to deduce closed-loop properties from open loop properties, the frequency response of the response ratio.

![[Pasted image 20251125182008.png]]

**Significance of the Point -1**
If the Nyquist locus passes through the point -1, the closed loop frequency response will become infinite at that frequency.

If $e(t)=\cos(\omega_1t)$, then in steady state we have $y(t)=-\cos(\omega_1t)$, by using the fact that the steady state response to a sinusoidal input is $|G(j\omega)|\cos(\omega t + \arg G(j\omega))$. 
However, this means that the input $r(t)=0$. Therefore we are getting sustained oscillation at frequency $\omega_1$ even with no input.

We must find the value of $\omega$ for which the frequency response of the return ratio is purely real. Then we can find this real value and see whether we cover the point -1 or not.

**Nyquist Stability Theorem**
If a feedback system has an asymptotically stable return ratio, $L(s)$, then the feedback system is also asymptotically stable if the Nyquist diagram of $L(j\omega)$ leaves the point -1 on its left

This is equivalent to saying that the closed loop transfer function is stable if all the poles of $\frac{L(s)}{1+L(s)}$ (the roots of $1+L(s)$ lie in the LHP)

**Gain and Phase Margins**
$L(j\omega)$ coming even close to -1 is undesirable because:
* It implies that a closed loop pole will be close to the imaginary axis and the closed loop system will be oscillatory as the denominator is still very large
* If $G(s)$ is the transfer function of an inaccurate model, then the true Nyquist diagram might actually encircle -1

Gain and phase margins measure how close the return ratio $L(j\omega)$ gets to -1
The gain margin measures how much the gain of the return ratio must be increases before the closed-loop system becomes unstable
The phase margin measures how much phase lag can be added to the return ratio before the closed-loop system becomes unstable
These can be both considered as design specifications

![[Pasted image 20251125184310.png]]
From the Bode plot:
![[Pasted image 20251125184326.png]]

Given a Nyquist diagram of $L(s)=kG(s)$, we can find the gain margin by just considering the $-1/k$ point instead of -1. 
This is because: $L(s)=kG(s)$. $-1=kG(s)$. $G(s)=-1/k$
![[Pasted image 20251125184604.png]]

The sensitivity is given by the transfer function between a disturbance and the output. In this case $\frac{1}{1+L(j\omega)}$
Good design aims for sensitivity reduction over an appropriate range of frequencies, so have the sensitivity much less than 1 for any omega less than a specified bandwidth.
The integral of the dB sensitivity from 0 to infinity has to equal 0, so if it's very negative at points it must be very positive at others.

The sensitivity needs to be less 1 up to $\omega_1$, the lowest frequency at which $|S(j\omega)|=1$. The closed loop system will respond to reference inputs at frequencies up to around $\omega_2$, as evident from the plot of the complementary sensitivity, $T(s)$. 
$S(s)+T(s)=1$
$\omega_2$ is the highest frequency at which $|T(j\omega)|=1$

When $|S(j\omega)|=1$, we are essentially saying that the distance from the locus to the point -1 is 1.
When $|T(j\omega)|=1$, we are essentially saying that the distance from the locus to -1 is the same as the distance of the origin to the locus.

$S(j\omega)$ is maximised when the distance from the locus to -1 is minimised, so perpendicular. This is bad for robustness and performance.
To find the maximum value of $T(j\omega)$, try some points around where $|1+L(j\omega)|$ is minimised

![[Pasted image 20251128112813.png]]

We close all our curves by plotting $-\omega$, which is just the conjugate of $L(j\omega)$

**Formal Nyquist Stability Criteria**
Consider a system with an asymptotically stable return ratio. In this case, the feedback system is asymptotically stable if and only if the point $-1+0j$ is not encircled by the full Nyquist diagram of $L(j\omega)$

* Encirclements must be added algebraically. If there is 1 clockwise and 1 anticlockwise encirclement then they add to 0.
* We must always close the Nyquist locus, even if we diverge to $\pm \infty$
* If L is unstable and has $n_p$ unstable poles, then the theorem must be modified as follows: "The feedback system is stable if and only if the full Nyquist diagram makes $n_p$ anticlockwise encirclements of -1"
* In pretty much all cases, the informal version of the theorem works

3 main things we want for our return ratio
1) Large gain at low frequencies, so that the steady state error is small, and so that the sensitivity is small
2) Small gain at high frequencies, so we ensure we don't encircle -1
3) Large gain margin at the point where the gain = 1

To achieve these, we can implement phase lag and phase lead compensators.

A phase lag compensator increases the gain at low frequencies, however it also introduces a phase lag, which may reduce the phase margin. This is not so bad as long as this phase lag occurs at frequencies much less than the cut off frequency, where gain is 1

A phase lead compensator decreases the gain at low frequencies and increases the gain at high frequencies, which is not what you want. However it also introduces a phase lead, which can be very useful for increasing our phase margin.

Phase lag compensator: $K(s)=k\frac{s+\alpha}{s+\beta}$ for $\beta < \alpha$ ($< \omega_c$ typically)
![[Pasted image 20251202151032.png]]

Phase lead compensator: $K(s)=k\frac{s+\alpha}{s+\beta}$ for $\alpha < \beta$ (and $\alpha < \omega_c < \beta$ typically)

Both have trade-offs, so we combine them to get the desired effect.
