$A \subseteq \Omega$ means A is a subset of omega

If $A\cap B = \oslash$, then A and B are disjoint, or independent

De Morgan's applies to set theory too

3 axioms of probability:
1) Non-negativity - probability is less than or equal to 0
2) Normalisation - the sum of probabilities over the entire sample space is 1
3) Additivity for disjoint events - $P[A\cup B]=P[A]+P[B]$ if $A\cap B = \oslash$

Sum rule: $P[A\cap B] + P[A\cap B^C]=P[A]$

Law of total probability: $P[A]=\sum_1^n P[A|B_k] \times P[B_k]$

Bayes' Theorem: Allows us to reverse the conditional probabilities:
$$P[B|A]=\frac{P[A|B] \times P[B]}{P[A]}$$
Independence: $P[A|B] = P[A]$

A discrete random variable X is a function defined on the outcomes of a random experiment
$X:\Omega \rightarrow \mathbb{X}$
The set of possible values $\mathbb{X}$ is called the support of X

**Probability Mass Function (PMF)**
Gives the probability that X is a certain value - $P_X(x)=P[X=x]$

**Cumulative Distribution Function (CDF)**
Gives $F_X(x)=P[X\leq x]$
$F_X$ is non-decreasing. For any $a < b$, $P[a < X \leq b] = F_X(b) - F_X(a)$

**Joint and Conditional Distributions**
The joint PMF of X and Y is $P_{XY}(x,y)=P[X=x \cap Y = y]$
The conditional PMF is defined as $P_{X|Y}(x|y)=\frac{P_{XY}(x,y)}{P_Y(y)}$

Marginalisation: Follows from the law of total probability:
$P_X(x)=\sum_y P_{XY}(x,y)$

**Independence**
Two random variables X and Y are independent if $P_{XY}(x,y)=P_X(x)P_Y(y)$ for all $x,y$
Mutually independent if $P_{X_1 ... X_n}=\prod P_{X_i}(x_i)$
Mutual independence is stronger than pairwise independence, which only requires independence for each pair

**Expectation**
$E[f(X,Y)]=\sum f(x,y)P_{X,Y}(x,y)$
Expectation is linear: $E[aX+bY]=aE[X]+bE[Y]$

Two random variables are said to be uncorrelated if: $E[XY]=E[X]E[Y]$
Independence implies uncorrelation, but uncorrelation does not imply independence

**Variance and Moments**
Expectation alone does not fully describe a function. We can use the central moments:
$E[(X-E[X])^n]$
* The second central moment is the variance, measuring spread $Var[X]=E[X^2]+E[X]^2$
* The third central moment is the asymmetry/ skewness
* The fourth central moment measures tailedness

**Entropy**
Entropy is introduced to quantify uncertainty or surprise
An event with probability 1 carries no information
Information content $I_X(x)=-\log_2 P_X (x)$
The entropy, measured in bits, is the expectation of the information content: $H[X]=-\sum_x P_X(x)\log_2 P_X (x)$

**Distributions**

**Bernoulli Distribution**
For when we have a yes or no, a binary random variable with probability p of being 1.
$E[X]=p$, $Var[X]=p(1-p)$
Entropy = $H_2(p)= -(1-p) \log_2 (1-p) - p \log_2 p$, the binary entropy function

**Geometric Distribution**
Success with probability p. Mutually independent trials. How long until success?
$P[\text{First success at k}]=(1-p)^{k-1}p$
This is called Geometric (2) in the databook. Geometric (1) is for 1st success after k fails
$E[X]=1/p$
$Var[X]=\frac{1-p}{p^2}$
Proof: $E[X^2]=\sum k^2 p q^{k-1}$
Trick 1: $k^2 = k(k-1) + k, \Rightarrow qp\sum k(k-1)q^{k-1}+\sum kpq^{k-1}$
Trick 2: $qp\frac{d^2}{dq^2}(\frac{q}{1-q})+\frac{1}{p}$. The rest is obvious
