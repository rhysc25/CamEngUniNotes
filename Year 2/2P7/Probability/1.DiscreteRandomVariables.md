$A \subseteq \Omega$ means A is a subset of omega

If $A\cap B = \oslash$, then A and B are disjoint, or independent

De Morgan's applies to set theory too

3 axioms of probability:
1) Non-negativity - probability is less than or equal to 0
2) Normalisation - the sum of probabilities over the entire sample space is 1
3) Additivity for disjoint events - $P[A\cup B]=P[A]+P[B]$ if $A\cap B = \oslash$

Sum rule: $P[A\cap B] + P[A\cap B^C]=P[A]$

Law of total probability: $P[A]=\sum_1^n P[A|B_k] \times P[B_k]$

Bayes' Theorem: Allows us to reverse the conditional probabilities:
$$P[B|A]=\frac{P[A|B] \times P[B]}{P[A]}$$
Independence: $P[A|B] = P[A]$

A discrete random variable X is a function defined on the outcomes of a random experiment
$X:\Omega \rightarrow \mathbb{X}$
The set of possible values $\mathbb{X}$ is called the support of X

**Probability Mass Function (PMF)**
Gives the probability that X is a certain value - $P_X(x)=P[X=x]$

**Cumulative Distribution Function (CDF)**
Gives $F_X(x)=P[X\leq x]$
$F_X$ is non-decreasing. For any $a < b$, $P[a < X \leq b] = F_X(b) - F_X(a)$

**Joint and Conditional Distributions**
The joint PMF of X and Y is $P_{XY}(x,y)=P[X=x \cap Y = y]$
The conditional PMF is defined as $P_{X|Y}(x|y)=\frac{P_{XY}(x,y)}{P_Y(y)}$

Marginalisation: Follows from the law of total probability:
$P_X(x)=\sum_y P_{XY}(x,y)$

**Independence**
Two random variables X and Y are independent if $P_{XY}(x,y)=P_X(x)P_Y(y)$ for all $x,y$
Mutually independent if $P_{X_1 ... X_n}=\prod P_{X_i}(x_i)$
Mutual independence is stronger than pairwise independence, which only requires independence for each pair

**Expectation**
$E[f(X,Y)]=\sum f(x,y)P_{X,Y}(x,y)$
Expectation is linear: $E[aX+bY]=aE[X]+bE[Y]$

Two random variables are said to be uncorrelated if: $E[XY]=E[X]E[Y]$
Independence implies uncorrelation, but uncorrelation does not imply independence

**Variance and Moments**
Expectation alone does not fully describe a function. We can use the central moments:
$E[(X-E[X])^n]$
* The second central moment is the variance, measuring spread $Var[X]=E[X^2]+E[X]^2$
* The third central moment is the asymmetry/ skewness
* The fourth central moment measures tailedness

**Entropy**
Entropy is introduced to quantify uncertainty or surprise
An event with probability 1 carries no information
Information content $I_X(x)=-\log_2 P_X (x)$
The entropy, measured in bits, is the expectation of the information content: $H[X]=-\sum_x P_X(x)\log_2 P_X (x)$

**Distributions**

**Bernoulli Distribution**
For when we have a yes or no, a binary random variable with probability p of being 1.
$E[X]=p$, $Var[X]=p(1-p)$
Entropy = $H_2(p)= -(1-p) \log_2 (1-p) - p \log_2 p$, the binary entropy function

**Geometric Distribution**
Success with probability p. Mutually independent trials. How long until success?
$P[\text{First success at k}]=(1-p)^{k-1}p$
This is called Geometric (2) in the databook. Geometric (1) is for 1st success after k fails
$E[X]=1/p$
$Var[X]=\frac{1-p}{p^2}$
Proof: $E[X^2]=\sum k^2 p q^{k-1}$
Trick 1: $k^2 = k(k-1) + k, \Rightarrow qp\sum k(k-1)q^{k-1}+\sum kpq^{k-1}$
Trick 2: $qp\frac{d^2}{dq^2}(\frac{q}{1-q})+\frac{1}{p}$. The rest is obvious
The entropy of the geometric distribution is the binary entropy function / h
It has an important memoryless property $P[X>m+n|X>m]=P[X>n]$. So if you do m trials, and restart your count, the distribution is the same, like starting from scratch.

**Binomial Distribution**
Trials are mutually independent. Same experiment n times. How many successes over n trials.
Permutations (order matters) (number of ways to place n objects into k $\leq$ n positions):
$^nP_k=\frac{n!}{(n-k)!}$
Combinations (order does not matter) (number of ways to choose k objects from n available):
$^nC_k=\frac{n!}{k!(n-k)!}$
$P[\text{k successes in n trials}]=^nC_k p^k (1-k)^{n-k}$
$E[X]=np$
$Var[X]=np(1-p)$
Both of these proven by considering $\sum ^nC_k p^k q^{n-k}=1$
The entropy function has no simple closed-form expression
The binomial distribution is like a sum of Bernoulli trials, where $X_i \approx Ber(p)$ acts as an indicator variable

**Poisson Distribution**
How many events occur in a given interval, given that the average rate of occurrence is $\lambda$
If we divide the interval into n subintervals, then it is essentially binomial, with $np=\lambda$
If we compute $\lim_{n\rightarrow \infty} ^nC_k p^k (1-p)^{n-k}$ with $p=\frac{\lambda}{n}$, this gives $\frac{\lambda^k e^{-\lambda}}{k!}$
The mean and the variance are both $\lambda$, which is a great way to identify the Poisson distribution. 
These are both proven by showing that $\sum \frac{\lambda^k e^{-\lambda}}{k!} = e^{-\lambda} \sum \frac{\lambda^k}{k!}=e^{-\lambda}e^{\lambda}=1$
The entropy function also has no simple closed-form expression

$B(n,p) \approx Pois(np)$ if n is large, p is small, and np is of moderate size

