Continuous random variables are those with a continuous support, whether than be finite or infinite.

**CDF - Cumulative Density Function**
In general, $P[X=x] = 0$ for a continuous random variable
We can still consider events corresponding to intervals
The CDF is $F_X(x)=P[X\leq x]$
$P[a<X\leq b]=F_X(b)-F_X(a)$

**PDF - Probability Density Function**
$$f_X(x)=\frac{dF_X(x)}{dx}$$
$f_X(x)dx\approx P[x<X\leq x+dx]$ = the probability of X in the infinitesimal interval $(x,x+dx]$
Probabilities can be found by integrating the PDF: $\int_a^b f_X (x)dx = F_X(b)-F_X(a)$
Normalisation: $\int_{-\infty}^{\infty}f_X (x) dx = 1$

**Joint PDF**
$$f_{XY}(x,y)=\frac{\partial^2 F_{XY}(x,y)}{\partial x \partial y}$$
where $F_{XY}(x,y)=P[X\leq x \cap Y \leq y]$
Marginalisation: $\int_{-\infty}^{\infty}f_{XY}(x,y)dy=f_X(x)$
Conditional PDF: $f_{X|Y}(x|y)=\frac{f_{XY}(x,y)}{f_{Y}(y)}$, and from this we can get Bayes theorem
And yes, this is conditioning on $Y=y$ exactly
Law of total probability: $f_X(x)=\int_{-\infty}^{\infty}f_{X|Y}(x|y)f_Y(y)dy$

Conditional independence: 
If X and Y are conditionally independent given $Z=z$ then $f_{XY|Z}(x,y|z)=f_{X|Z}(x|z)f_{Y|Z}(Y|Z)$ 
This is equivalent to saying $f_{X|YZ}(x|yz)=f_{X|Z}(x|z)$

**Expectation and Moments of a PDF**
The PDF can be used to compute expectations
$E[X^n]$ is the nth moment
$E[(X-E[X])^n]$ is the nth central moment
Mean/ first moment = $\int_{-\infty}^{\infty}xf_X(x)dx$
Standard deviation, $\sigma = \sqrt{Var[X]}$

$E[X|Y=y]=\int_{-\infty}^{\infty}x f_{X|Y}(x,y) dx$
In general, one can define a PDF for a discrete random variable using the delta function. If X discrete with PMF $P_X$ and support $\mathbb{X}$, then:
$f_X(x)=\sum P_X(k)\delta(x-k)$

**Characteristics of a PDF**
* Mode: value of x at which $f_X(x)$ is max
* Median: value $Q_{1/2}$ such that $F_X(Q_{1/2})=1/2$, so splits the area under the PDF into two equal parts. There is also an upper and lower quartile, and an interquartile range
* Skewness: $\frac{E[(X-E[X])^3]}{\sigma^3}$. Positive skew indicates a longer right tail

**The Exponential Distribution**
The time or distance to the next event in a Poisson process
$f_T(t)=\lambda e^{-\lambda t}$
The mean is $1/\lambda$ and the variance is $1/\lambda^2$
For a quantile, $Q_p = -\frac{\ln(1-p)}{\lambda}$
This is memoryless. $P[T>t]=e^{-\lambda t}$
If we instead had discrete time steps, we would use geometric distribution

**The Normal Distribution**
A quantity results from many independent contributions
The individual effects have finite mean and variance
The aggregate random variable follows a normal distribution

Central Limit Theorem
For independent variables, if we sum all the differences from the mean, and normalise by dividing by $\sqrt{n}$. As $n \rightarrow \infty$, we get the normal distribution: $$f_{X}(X)=\frac{1}{\sqrt{2\pi \sigma^2}}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$
The standard normal is found by $Y = (X-\mu)/\sigma$. $Y$ ~ $N(0,1)$
$\Phi(y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^y e^{x^2/2}dx$
$\Phi(-y)=1-\Phi(y)$
The probability of X being within m standard deviations of the mean is $P[|X-\mu|\leq m\sigma]=2\Phi(m)-1$

To prove normalisation for the normal distribution, square the integral and split it into two.
Quantile: $Q_p=F_X^{-1}(p)=\mu + \sigma \Phi^{-1}(p)$

**Multivariate Normal Distribution**
Generalises the normal distribution into random vectors. $f_{X_1,...,X_n}(x_1,...,x_n)=f_{\textbf{X}}(\textbf{x})$
$\textbf{X}$ ~ $N(\mu, \Sigma)$ where $\Sigma$ is the symmetric n x n covariance matrix 
$\Sigma_{ij}=Cov[X_i,X_j]$
$Cov[X,Y]=Cov[Y,X]$ and $Cov[X,X]=Var[X]$

The Pearson's correlation coefficient is given be $\rho=\frac{Cox[X_1,X_2]}{\sigma_1 \sigma_2}$

The marginals are also normal
Zero correlation, covariance = 0, implies independence
The conditional PDF is normal

**Beta Distribution**
Say we perform mutually independent Bernoulli trials n times where p is unknown.
The random probability follows a beta distribution, depending on the value k, the number of successes we get out of n trials.

If you don't have any information, first assume p is uniformly distributed in $[0,1]$
After the trials we have: $$f_{p|k}(p|k)=\frac{(n+1)!}{k!(n-k)!}p^k (1-p)^{n-k}$$
Bayes' theorem updates our beliefs about am unknown probability
The gamma function extends the factorial to non-integer values
$\Gamma (a) = \int_0^{\infty}\zeta^{a-1}e^{-\zeta}d\zeta$
When a is a positive integer this reduces to $\Gamma(a)=(a-1)!$
Hence: $$f_{p|k}(p|k)=\frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}x^{\alpha - 1}(1-x)^{\beta-1}$$
Where $\alpha = k+1$ and $\beta=n-k+1$ and can take any real values

The expectation and variance of this are in the data book. The mode is the proportion of success, $k/n$
